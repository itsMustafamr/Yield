{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6ecbd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabpfn import TabPFNRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# 1. Load\n",
    "train = pd.read_csv('train.csv')\n",
    "val   = pd.read_csv('val.csv')\n",
    "test  = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fa53d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba0d6d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Load raw CSVs\n",
    "train = pd.read_csv('train.csv')\n",
    "val   = pd.read_csv('val.csv')\n",
    "test  = pd.read_csv('test.csv')\n",
    "\n",
    "# 2. Impute missing values (TRAIN-only statistics)\n",
    "mg_mode    = train['MG'].mode()[0]\n",
    "lon_med    = train['Longitude'].median()\n",
    "mean_cols  = ['Lodging','PlantHeight','SeedSize','Protein','Oil']\n",
    "mean_vals  = train[mean_cols].mean()\n",
    "\n",
    "for df in (train, val, test):\n",
    "    df['MG']        = df['MG'].fillna(mg_mode)\n",
    "    df['Longitude'] = df['Longitude'].fillna(lon_med)\n",
    "    for c in mean_cols:\n",
    "        df[c]      = df[c].fillna(mean_vals[c])\n",
    "\n",
    "# 3. Feature definitions\n",
    "temporal_feats = ['MaxTemp','MinTemp','AvgTemp','AvgHumidity','Precipitation','Radiation']\n",
    "static_feats   = ['Latitude','Longitude','Row.Spacing']\n",
    "plant_feats    = ['Lodging','PlantHeight','SeedSize','Protein','Oil']\n",
    "cluster_feats  = [f'Cluster_{i}' for i in range(40)]\n",
    "\n",
    "# 4. Aggregation function\n",
    "def aggregate_sequences(df, target='Yield', agg_target='mean'):\n",
    "    agg = {}\n",
    "    # temporal: mean & std\n",
    "    for f in temporal_feats:\n",
    "        agg[f'{f}_mean'] = (f, 'mean')\n",
    "        agg[f'{f}_std']  = (f, 'std')\n",
    "    # static geography: first\n",
    "    for f in static_feats:\n",
    "        agg[f] = (f, 'first')\n",
    "    # plant: MG by mode, others by first\n",
    "    agg['MG'] = ('MG', lambda x: x.mode().iloc[0])\n",
    "    for f in plant_feats:\n",
    "        agg[f] = (f, 'first')\n",
    "    # clusters: mean & std\n",
    "    for f in cluster_feats:\n",
    "        agg[f'{f}_mean'] = (f, 'mean')\n",
    "        agg[f'{f}_std']  = (f, 'std')\n",
    "    # target\n",
    "    if agg_target=='mean':\n",
    "        agg[target] = (target, 'mean')\n",
    "    else:\n",
    "        agg[target] = (target, lambda x: x.iloc[-1])\n",
    "    return df.groupby('TimeSeriesLabel').agg(**agg).reset_index(drop=True)\n",
    "\n",
    "train_agg = aggregate_sequences(train)\n",
    "val_agg   = aggregate_sequences(val)\n",
    "test_agg  = aggregate_sequences(test)\n",
    "\n",
    "# 5. Split into X/y\n",
    "X_train = train_agg.drop('Yield', axis=1)\n",
    "y_train = train_agg['Yield']\n",
    "\n",
    "X_val   = val_agg.drop('Yield',   axis=1)\n",
    "y_val   = val_agg['Yield']\n",
    "\n",
    "X_test  = test_agg.drop('Yield',  axis=1)\n",
    "y_test  = test_agg['Yield']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "385f9d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Quantization helpers\n",
    "def fit_quantizer(df, b):\n",
    "    levels = 2**b\n",
    "    params = {}\n",
    "    for col in df.columns:\n",
    "        xmin, xmax = df[col].min(), df[col].max()\n",
    "        if xmax==xmin: continue\n",
    "        delta = (xmax-xmin)/(levels-1)\n",
    "        params[col] = (xmin, delta)\n",
    "    return params\n",
    "\n",
    "def apply_quantizer(df, params):\n",
    "    df_q = df.copy()\n",
    "    for col, (xmin, delta) in params.items():\n",
    "        df_q[col] = xmin + delta * np.round((df_q[col]-xmin)/delta)\n",
    "    return df_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3df45a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b=8 → Val R²: 0.7772, RMSE: 7.0752\n",
      "b=8 → Test R²: 0.7790, RMSE: 6.9979\n"
     ]
    }
   ],
   "source": [
    "b = 8\n",
    "q_params = fit_quantizer(X_train, b)\n",
    "\n",
    "Xtr_q = apply_quantizer(X_train, q_params).to_numpy()\n",
    "Xvl_q = apply_quantizer(X_val,   q_params).to_numpy()\n",
    "Xte_q = apply_quantizer(X_test,  q_params).to_numpy()\n",
    "\n",
    "# Subsample up to 10k for fitting (reproducible)\n",
    "n_sub = min(10_000, len(Xtr_q))\n",
    "rng = np.random.RandomState(42)\n",
    "idx = rng.choice(len(Xtr_q), size=n_sub, replace=False)\n",
    "Xsub, ysub = Xtr_q[idx], y_train.to_numpy()[idx]\n",
    "\n",
    "model = TabPFNRegressor()\n",
    "model.fit(Xsub, ysub)\n",
    "\n",
    "# Evaluate\n",
    "for name, Xq, y in [\n",
    "    ('Val',  Xvl_q, y_val.to_numpy()),\n",
    "    ('Test', Xte_q, y_test.to_numpy())\n",
    "]:\n",
    "    preds = model.predict(Xq)\n",
    "    print(f\"b={b} → {name} R²: {r2_score(y, preds):.4f}, \"\n",
    "          f\"RMSE: {root_mean_squared_error(y, preds):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3a71b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Real MLP Baseline =====\n",
      "\n",
      "b=2\n",
      "Val  R²: -1.0954, RMSE: 21.6975\n",
      "Test R²: -1.9444, RMSE: 25.5457\n",
      "\n",
      "b=4\n",
      "Val  R²: 0.5136, RMSE: 10.4539\n",
      "Test R²: 0.5296, RMSE: 10.2111\n",
      "\n",
      "b=6\n",
      "Val  R²: 0.5120, RMSE: 10.4705\n",
      "Test R²: 0.5246, RMSE: 10.2650\n",
      "\n",
      "b=8\n",
      "Val  R²: 0.4945, RMSE: 10.6576\n",
      "Test R²: 0.5080, RMSE: 10.4423\n",
      "\n",
      "b=16\n",
      "Val  R²: 0.5003, RMSE: 10.5953\n",
      "Test R²: 0.5153, RMSE: 10.3651\n",
      "\n",
      "b=32\n",
      "Val  R²: 0.4794, RMSE: 10.8153\n",
      "Test R²: 0.4946, RMSE: 10.5842\n",
      "\n",
      "b=64\n",
      "Val  R²: 0.5123, RMSE: 10.4679\n",
      "Test R²: 0.5196, RMSE: 10.3189\n",
      "\n",
      "b=128\n",
      "Val  R²: 0.4976, RMSE: 10.6243\n",
      "Test R²: 0.5231, RMSE: 10.2804\n",
      "\n",
      "b=256\n",
      "Val  R²: 0.5219, RMSE: 10.3639\n",
      "Test R²: 0.5368, RMSE: 10.1318\n"
     ]
    }
   ],
   "source": [
    "# 8. Baseline: Real MLP for tabular regression (PyTorch preferred, sklearn fallback)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# robust RMSE import (sklearn versions differ)\n",
    "try:\n",
    "    from sklearn.metrics import root_mean_squared_error as _rmse\n",
    "    def RMSE(y, p): return _rmse(y, p)\n",
    "except Exception:\n",
    "    def RMSE(y, p): return mean_squared_error(y, p, squared=False)\n",
    "\n",
    "# -----------------------------\n",
    "# Try PyTorch first\n",
    "# -----------------------------\n",
    "_USE_TORCH = True\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "except Exception:\n",
    "    _USE_TORCH = False\n",
    "\n",
    "def mlp_eval_print(name, y_true, y_pred):\n",
    "    print(f\"{name} R²: {r2_score(y_true, y_pred):.4f}, RMSE: {RMSE(y_true, y_pred):.4f}\")\n",
    "\n",
    "if _USE_TORCH:\n",
    "    # ---- PyTorch MLP ----\n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self, d_in, widths=(512, 256, 128), dropout=0.1):\n",
    "            super().__init__()\n",
    "            layers = []\n",
    "            prev = d_in\n",
    "            for w in widths:\n",
    "                layers += [nn.Linear(prev, w), nn.ReLU(), nn.Dropout(dropout)]\n",
    "                prev = w\n",
    "            layers += [nn.Linear(prev, 1)]\n",
    "            self.net = nn.Sequential(*layers)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.net(x).squeeze(-1)\n",
    "\n",
    "    def train_mlp_torch(Xtr, ytr, Xvl, yvl, *,\n",
    "                        epochs=200, lr=1e-3, batch_size=1024, patience=20, seed=42):\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        scaler = StandardScaler().fit(Xtr)\n",
    "        Xtr_s = scaler.transform(Xtr).astype(np.float32)\n",
    "        Xvl_s = scaler.transform(Xvl).astype(np.float32)\n",
    "\n",
    "        Xtr_t = torch.from_numpy(Xtr_s)\n",
    "        ytr_t = torch.from_numpy(ytr.astype(np.float32))\n",
    "        Xvl_t = torch.from_numpy(Xvl_s)\n",
    "        yvl_t = torch.from_numpy(yvl.astype(np.float32))\n",
    "\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = MLP(d_in=Xtr_t.shape[1]).to(device)\n",
    "        opt = optim.AdamW(model.parameters(), lr=lr)\n",
    "        loss_fn = nn.MSELoss()\n",
    "\n",
    "        def batches(X, y, bs):\n",
    "            idx = np.random.permutation(len(X))\n",
    "            for i in range(0, len(X), bs):\n",
    "                j = idx[i:i+bs]\n",
    "                yield X[j], y[j]\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        best_state = None\n",
    "        bad = 0\n",
    "\n",
    "        for ep in range(1, epochs+1):\n",
    "            model.train()\n",
    "            for xb, yb in batches(Xtr_t, ytr_t, batch_size):\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                opt.zero_grad()\n",
    "                pred = model(xb)\n",
    "                loss = loss_fn(pred, yb)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "            # val\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_pred = model(Xvl_t.to(device)).cpu()\n",
    "                val_loss = loss_fn(val_pred, yvl_t).item()\n",
    "\n",
    "            if val_loss < best_loss - 1e-6:\n",
    "                best_loss = val_loss\n",
    "                best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "                bad = 0\n",
    "            else:\n",
    "                bad += 1\n",
    "                if bad >= patience:\n",
    "                    break\n",
    "\n",
    "        if best_state is not None:\n",
    "            model.load_state_dict(best_state)\n",
    "\n",
    "        return model, scaler, device\n",
    "\n",
    "    def predict_mlp_torch(model, scaler, device, X):\n",
    "        Xs = scaler.transform(X).astype(np.float32)\n",
    "        Xt = torch.from_numpy(Xs).to(device)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = model(Xt).cpu().numpy()\n",
    "        return preds\n",
    "\n",
    "else:\n",
    "    # ---- sklearn fallback ----\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "    def train_mlp_sklearn(Xtr, ytr, Xvl, yvl, *, seed=42):\n",
    "        # Fit scaler on train, apply to all\n",
    "        scaler = StandardScaler().fit(Xtr)\n",
    "        Xtr_s = scaler.transform(Xtr)\n",
    "        Xvl_s = scaler.transform(Xvl)\n",
    "\n",
    "        # Early stopping internal to sklearn (uses a split from training data)\n",
    "        mlp = MLPRegressor(hidden_layer_sizes=(512, 256, 128),\n",
    "                           activation='relu',\n",
    "                           solver='adam',\n",
    "                           max_iter=500,\n",
    "                           learning_rate_init=1e-3,\n",
    "                           early_stopping=True,\n",
    "                           n_iter_no_change=20,\n",
    "                           random_state=seed)\n",
    "        mlp.fit(Xtr_s, ytr)\n",
    "        return mlp, scaler\n",
    "\n",
    "    def predict_mlp_sklearn(model, scaler, X):\n",
    "        return model.predict(scaler.transform(X))\n",
    "\n",
    "# -----------------------------\n",
    "# Run MLP over the same bit-depths\n",
    "# -----------------------------\n",
    "print(\"\\n===== Real MLP Baseline =====\")\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "for b in [2,4,6,8,16,32,64,128,256]:\n",
    "    # Quantize using your existing helpers/params\n",
    "    q_params = fit_quantizer(X_train, b)\n",
    "    Xtr_q = apply_quantizer(X_train, q_params).to_numpy()\n",
    "    Xvl_q = apply_quantizer(X_val,   q_params).to_numpy()\n",
    "    Xte_q = apply_quantizer(X_test,  q_params).to_numpy()\n",
    "\n",
    "    # Subsample 10k for parity with TabPFN\n",
    "    n_sub = min(10_000, len(Xtr_q))\n",
    "    idx = rng.choice(len(Xtr_q), size=n_sub, replace=False)\n",
    "    Xsub, ysub = Xtr_q[idx], y_train.to_numpy()[idx]\n",
    "\n",
    "    print(f\"\\nb={b}\")\n",
    "    if _USE_TORCH:\n",
    "        model, scaler, device = train_mlp_torch(Xsub, ysub, Xvl_q, y_val.to_numpy(),\n",
    "                                                epochs=200, lr=1e-3, batch_size=1024, patience=20)\n",
    "        pred_v = predict_mlp_torch(model, scaler, device, Xvl_q)\n",
    "        pred_t = predict_mlp_torch(model, scaler, device, Xte_q)\n",
    "    else:\n",
    "        model, scaler = train_mlp_sklearn(Xsub, ysub, Xvl_q, y_val.to_numpy())\n",
    "        pred_v = predict_mlp_sklearn(model, scaler, Xvl_q)\n",
    "        pred_t = predict_mlp_sklearn(model, scaler, Xte_q)\n",
    "\n",
    "    mlp_eval_print(\"Val \", y_val.to_numpy(), pred_v)\n",
    "    mlp_eval_print(\"Test\", y_test.to_numpy(), pred_t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabpfn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
