{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7c435d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86eba8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Load\n",
    "train = pd.read_csv('train.csv')\n",
    "val   = pd.read_csv('val.csv')\n",
    "test  = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de79efff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Compute fill‐values on TRAIN only\n",
    "mg_mode      = train['MG'].mode()[0]           # mode for categorical MG\n",
    "lon_median   = train['Longitude'].median()     # median for longitude\n",
    "mean_cols    = ['Lodging','PlantHeight','SeedSize','Protein','Oil']\n",
    "mean_values  = train[mean_cols].mean()         # means for plant characteristics\n",
    "\n",
    "# 2. Fill missing in train/val/test\n",
    "for df in (train, val, test):\n",
    "    df['MG']        = df['MG'].fillna(mg_mode)\n",
    "    df['Longitude'] = df['Longitude'].fillna(lon_median)\n",
    "    for col in mean_cols:\n",
    "        df[col]     = df[col].fillna(mean_values[col])\n",
    "\n",
    "# 3. Define your features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "474c3a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure these are defined at top‐level:\n",
    "temporal_feats = ['MaxTemp','MinTemp','AvgTemp','AvgHumidity','Precipitation','Radiation']\n",
    "static_feats   = ['Latitude','Longitude','Row.Spacing']\n",
    "plant_feats    = ['Lodging','PlantHeight','SeedSize','Protein','Oil']\n",
    "cluster_feats  = [f'Cluster_{i}' for i in range(40)]\n",
    "\n",
    "def aggregate_sequences(df, target='Yield', agg_target='mean'):\n",
    "    agg_dict = {}\n",
    "\n",
    "    # 1. temporal: mean & std\n",
    "    for feat in temporal_feats:\n",
    "        agg_dict[f'{feat}_mean'] = (feat, 'mean')\n",
    "        agg_dict[f'{feat}_std']  = (feat, 'std')\n",
    "\n",
    "    # 2. static geography: take first (constant per sequence)\n",
    "    for feat in static_feats:\n",
    "        agg_dict[feat] = (feat, 'first')\n",
    "\n",
    "    # 3. plant features:\n",
    "    #    - MG (categorical) → mode  \n",
    "    agg_dict['MG'] = ('MG', lambda x: x.mode().iloc[0])\n",
    "    #    - Lodging, PlantHeight, SeedSize, Protein, Oil → first\n",
    "    for feat in plant_feats:\n",
    "        agg_dict[feat] = (feat, 'first')\n",
    "\n",
    "    # 4. cluster indicators: proportion of time in each cluster + variability\n",
    "    for feat in cluster_feats:\n",
    "        agg_dict[f'{feat}_mean'] = (feat, 'mean')\n",
    "        agg_dict[f'{feat}_std']  = (feat, 'std')\n",
    "\n",
    "    # 5. target: mean or final\n",
    "    if agg_target == 'mean':\n",
    "        agg_dict[target] = (target, 'mean')\n",
    "    elif agg_target == 'final':\n",
    "        agg_dict[target] = (target, lambda x: x.iloc[-1])\n",
    "    else:\n",
    "        raise ValueError(\"agg_target must be 'mean' or 'final'\")\n",
    "\n",
    "    # apply the aggregation\n",
    "    grouped = df.groupby('TimeSeriesLabel').agg(**agg_dict)\n",
    "    return grouped.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2709cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_agg = aggregate_sequences(train, agg_target='mean')\n",
    "val_agg   = aggregate_sequences(val,   agg_target='mean')\n",
    "test_agg  = aggregate_sequences(test,  agg_target='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33c7d84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Split features / target\n",
    "X_train = train_agg.drop('Yield', axis=1)\n",
    "y_train = train_agg['Yield']\n",
    "X_val   = val_agg.drop('Yield',   axis=1)\n",
    "y_val   = val_agg['Yield']\n",
    "X_test  = test_agg.drop('Yield',  axis=1)\n",
    "y_test  = test_agg['Yield']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e922a1be",
   "metadata": {},
   "outputs": [
    {
     "ename": "InductorError",
     "evalue": "RuntimeError: Compiler: cl is not found.\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInductorError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# 4 . evaluate (keep context_size ≤ 8192 so SDPA never overflows)\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, X_np, y_np \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[32m     20\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mVal\u001b[39m\u001b[33m\"\u001b[39m,  X_val.to_numpy(),  y_val.to_numpy()),\n\u001b[32m     21\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mTest\u001b[39m\u001b[33m\"\u001b[39m, X_test.to_numpy(), y_test.to_numpy())\n\u001b[32m     22\u001b[39m ]:\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     preds = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_ensembles\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     rmse  = mean_squared_error(y_np, preds, squared=\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# root-MSE\u001b[39;00m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m R²   : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr2_score(y_np,\u001b[38;5;250m \u001b[39mpreds)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\TabDPT\\src\\tabdpt\\regressor.py:94\u001b[39m, in \u001b[36mTabDPTRegressor.predict\u001b[39m\u001b[34m(self, X, n_ensembles, context_size, seed)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\n\u001b[32m     87\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     88\u001b[39m     X: np.ndarray,\n\u001b[32m   (...)\u001b[39m\u001b[32m     91\u001b[39m     seed: \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     92\u001b[39m ):\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_ensembles == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     96\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._ensemble_predict(X, n_ensembles=n_ensembles, context_size=context_size)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabdpt_env\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\TabDPT\\src\\tabdpt\\regressor.py:69\u001b[39m, in \u001b[36mTabDPTRegressor._predict\u001b[39m\u001b[34m(self, X, context_size, seed)\u001b[39m\n\u001b[32m     67\u001b[39m     X_eval = test_x[start:end]\n\u001b[32m     68\u001b[39m     X_eval = pad_x(X_eval.unsqueeze(\u001b[32m1\u001b[39m), \u001b[38;5;28mself\u001b[39m.max_features).to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx_src\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_nni\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_eval\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_src\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_nni\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     pred_list.append(pred.squeeze())\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(pred_list).squeeze().detach().cpu().float().numpy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabdpt_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabdpt_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabdpt_env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:663\u001b[39m, in \u001b[36m_TorchDynamoContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    660\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ShortenTraceback \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    661\u001b[39m     \u001b[38;5;66;03m# Failures in the backend likely don't have useful\u001b[39;00m\n\u001b[32m    662\u001b[39m     \u001b[38;5;66;03m# data in the TorchDynamo frames, so we strip them out.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.remove_dynamo_frames() \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# see TORCHDYNAMO_VERBOSE=1\u001b[39;00m\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    665\u001b[39m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[32m    666\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabdpt_env\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:760\u001b[39m, in \u001b[36m_compile_fx_inner\u001b[39m\u001b[34m(gm, example_inputs, **graph_kwargs)\u001b[39m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    759\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m760\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InductorError(e, currentframe()).with_traceback(\n\u001b[32m    761\u001b[39m         e.__traceback__\n\u001b[32m    762\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    763\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    764\u001b[39m     TritonBundler.end_compile()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabdpt_env\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:745\u001b[39m, in \u001b[36m_compile_fx_inner\u001b[39m\u001b[34m(gm, example_inputs, **graph_kwargs)\u001b[39m\n\u001b[32m    743\u001b[39m TritonBundler.begin_compile()\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m     mb_compiled_graph = \u001b[43mfx_codegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgraph_kwargs\u001b[49m\n\u001b[32m    747\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    748\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m mb_compiled_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    749\u001b[39m     mb_compiled_graph._time_taken_ns = time.time_ns() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabdpt_env\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:1295\u001b[39m, in \u001b[36mfx_codegen_and_compile\u001b[39m\u001b[34m(gm, example_inputs, inputs_to_check, **graph_kwargs)\u001b[39m\n\u001b[32m   1291\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompile_fx_subproc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _SubprocessFxCompile\n\u001b[32m   1293\u001b[39m     scheme = _SubprocessFxCompile()\n\u001b[32m-> \u001b[39m\u001b[32m1295\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscheme\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcodegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabdpt_env\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:1197\u001b[39m, in \u001b[36m_InProcessFxCompile.codegen_and_compile\u001b[39m\u001b[34m(self, gm, example_inputs, inputs_to_check, graph_kwargs)\u001b[39m\n\u001b[32m   1184\u001b[39m             compiled_fn = AotCodeCompiler.compile(\n\u001b[32m   1185\u001b[39m                 graph,\n\u001b[32m   1186\u001b[39m                 wrapper_code.value,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1194\u001b[39m                 ],\n\u001b[32m   1195\u001b[39m             )\n\u001b[32m   1196\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1197\u001b[39m         compiled_fn = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.call\n\u001b[32m   1199\u001b[39m num_bytes, nodes_num_elem, node_runtimes = graph.count_bytes()\n\u001b[32m   1200\u001b[39m metrics.num_bytes_accessed += num_bytes\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabdpt_env\\Lib\\site-packages\\torch\\_inductor\\graph.py:2083\u001b[39m, in \u001b[36mGraphLowering.compile_to_module\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2076\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompile_to_module\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> ModuleType:\n\u001b[32m   2077\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[32m   2078\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mGraphLowering.compile_to_module\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2079\u001b[39m         phase_name=\u001b[33m\"\u001b[39m\u001b[33mcode_gen\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2080\u001b[39m         log_pt2_compile_event=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   2081\u001b[39m         dynamo_compile_column_us=\u001b[33m\"\u001b[39m\u001b[33minductor_code_gen_cumulative_compile_time_us\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2082\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m2083\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabdpt_env\\Lib\\site-packages\\torch\\_inductor\\graph.py:2091\u001b[39m, in \u001b[36mGraphLowering._compile_to_module\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2086\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcodecache\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PyCodeCache\n\u001b[32m   2088\u001b[39m \u001b[38;5;66;03m# Currently, if we're here, we don't have to worry about the kernel code, which\u001b[39;00m\n\u001b[32m   2089\u001b[39m \u001b[38;5;66;03m# is only available in AOTInductor mode.\u001b[39;00m\n\u001b[32m   2090\u001b[39m wrapper_code, _ = (\n\u001b[32m-> \u001b[39m\u001b[32m2091\u001b[39m     \u001b[38;5;28mself\u001b[39m.codegen_with_cpp_wrapper() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cpp_wrapper \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcodegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2092\u001b[39m )\n\u001b[32m   2093\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.triton.autotune_at_compile_time:\n\u001b[32m   2094\u001b[39m     tuning_code = (\n\u001b[32m   2095\u001b[39m         \u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m   2096\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[33mCompile-time auto-tuning block: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2099\u001b[39m         + \u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m   2100\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabdpt_env\\Lib\\site-packages\\torch\\_inductor\\graph.py:2002\u001b[39m, in \u001b[36mGraphLowering.codegen\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1999\u001b[39m V.debug.draw_orig_fx_graph(\u001b[38;5;28mself\u001b[39m.orig_gm, \u001b[38;5;28mself\u001b[39m.scheduler.nodes)\n\u001b[32m   2001\u001b[39m \u001b[38;5;28mself\u001b[39m.wrapper_code.push_codegened_graph(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2002\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcodegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2004\u001b[39m log.debug(\n\u001b[32m   2005\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFinished codegen for all nodes. The list of kernel names available: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   2006\u001b[39m     V.graph.all_codegen_kernel_names,\n\u001b[32m   2007\u001b[39m )\n\u001b[32m   2008\u001b[39m \u001b[38;5;66;03m# Dump provenance artifacts for debugging trace\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabdpt_env\\Lib\\site-packages\\torch\\_inductor\\scheduler.py:4135\u001b[39m, in \u001b[36mScheduler.codegen\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   4130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcodegen\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4131\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[33m\"\u001b[39m\u001b[33mScheduler.codegen\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   4132\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m   4133\u001b[39m             \u001b[38;5;28mself\u001b[39m._codegen_partitions()\n\u001b[32m   4134\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m torch._inductor.config.graph_partition\n\u001b[32m-> \u001b[39m\u001b[32m4135\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_codegen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4136\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabdpt_env\\Lib\\site-packages\\torch\\_inductor\\scheduler.py:4264\u001b[39m, in \u001b[36mScheduler._codegen\u001b[39m\u001b[34m(self, nodes)\u001b[39m\n\u001b[32m   4262\u001b[39m     backend.codegen_combo_kernel(node)\n\u001b[32m   4263\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, (FusedSchedulerNode, SchedulerNode)):\n\u001b[32m-> \u001b[39m\u001b[32m4264\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcodegen_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4265\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4266\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, NopKernelSchedulerNode)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabdpt_env\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:4982\u001b[39m, in \u001b[36mCppScheduling.codegen_node\u001b[39m\u001b[34m(self, node)\u001b[39m\n\u001b[32m   4979\u001b[39m kernel_group = \u001b[38;5;28mself\u001b[39m.kernel_group\n\u001b[32m   4981\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, OuterLoopFusedSchedulerNode):\n\u001b[32m-> \u001b[39m\u001b[32m4982\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcodegen_outer_loop_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4983\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4984\u001b[39m     nodes: \u001b[38;5;28mlist\u001b[39m[SchedulerNode] = node.get_nodes()  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabdpt_env\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:4956\u001b[39m, in \u001b[36mCppScheduling.codegen_outer_loop_node\u001b[39m\u001b[34m(self, node)\u001b[39m\n\u001b[32m   4949\u001b[39m         kernel_group.finalize_kernel(\n\u001b[32m   4950\u001b[39m             outer_fusion_cpp_kernel_proxy,\n\u001b[32m   4951\u001b[39m             [*itertools.chain.from_iterable(nodes_list)],\n\u001b[32m   4952\u001b[39m         )\n\u001b[32m   4954\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4956\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtry_outer_loop_fusion_with_local_buf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   4957\u001b[39m     \u001b[38;5;66;03m# Reset generated_cpp_vec_kernel_count to codegen again\u001b[39;00m\n\u001b[32m   4958\u001b[39m     metrics.generated_cpp_vec_kernel_count = generated_cpp_vec_kernel_count\n\u001b[32m   4959\u001b[39m     cpp_kernel_proxy_list.clear()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabdpt_env\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:4927\u001b[39m, in \u001b[36mCppScheduling.codegen_outer_loop_node.<locals>.try_outer_loop_fusion_with_local_buf\u001b[39m\u001b[34m(node)\u001b[39m\n\u001b[32m   4925\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _node \u001b[38;5;129;01min\u001b[39;00m node.get_outer_nodes():\n\u001b[32m   4926\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_node, (FusedSchedulerNode, SchedulerNode))\n\u001b[32m-> \u001b[39m\u001b[32m4927\u001b[39m     cpp_kernel_proxy = \u001b[43mCppKernelProxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4928\u001b[39m     cpp_kernel_proxy.codegen_nodes(_node.get_nodes())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   4929\u001b[39m     cpp_kernel_proxy_list.append(cpp_kernel_proxy)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabdpt_env\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:3734\u001b[39m, in \u001b[36mCppKernelProxy.__init__\u001b[39m\u001b[34m(self, kernel_group)\u001b[39m\n\u001b[32m   3732\u001b[39m \u001b[38;5;28mself\u001b[39m.loop_nest = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3733\u001b[39m \u001b[38;5;28mself\u001b[39m.call_ranges = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3734\u001b[39m \u001b[38;5;28mself\u001b[39m.picked_vec_isa: cpu_vec_isa.VecISA = \u001b[43mcpu_vec_isa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpick_vec_isa\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3735\u001b[39m \u001b[38;5;28mself\u001b[39m.kernels: \u001b[38;5;28mlist\u001b[39m[CppKernel] = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabdpt_env\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:418\u001b[39m, in \u001b[36mpick_vec_isa\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.is_fbcode() \u001b[38;5;129;01mand\u001b[39;00m (platform.machine() \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mx86_64\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAMD64\u001b[39m\u001b[33m\"\u001b[39m]):\n\u001b[32m    416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m VecAVX2()\n\u001b[32m--> \u001b[39m\u001b[32m418\u001b[39m _valid_vec_isa_list: \u001b[38;5;28mlist\u001b[39m[VecISA] = \u001b[43mvalid_vec_isa_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _valid_vec_isa_list:\n\u001b[32m    420\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_vec_isa\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabdpt_env\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:405\u001b[39m, in \u001b[36mvalid_vec_isa_list\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    402\u001b[39m \u001b[33;03m    arch value is x86_64 on Linux, and the value is AMD64 on Windows.\u001b[39;00m\n\u001b[32m    403\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    404\u001b[39m     _cpu_supported_x86_isa = x86_isa_checker()\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m     \u001b[43misa_list\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m        \u001b[49m\u001b[43misa\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43misa\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msupported_vec_isa_list\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mflag\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_cpu_supported_x86_isa\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mflag\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43misa\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43misa\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m isa_list\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabdpt_env\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:405\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    402\u001b[39m \u001b[33;03m    arch value is x86_64 on Linux, and the value is AMD64 on Windows.\u001b[39;00m\n\u001b[32m    403\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    404\u001b[39m     _cpu_supported_x86_isa = x86_isa_checker()\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m     isa_list.extend(\n\u001b[32m    406\u001b[39m         isa\n\u001b[32m    407\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m isa \u001b[38;5;129;01min\u001b[39;00m supported_vec_isa_list\n\u001b[32m    408\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(flag \u001b[38;5;129;01min\u001b[39;00m _cpu_supported_x86_isa \u001b[38;5;28;01mfor\u001b[39;00m flag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(isa).split()) \u001b[38;5;129;01mand\u001b[39;00m isa\n\u001b[32m    409\u001b[39m     )\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m isa_list\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabdpt_env\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:142\u001b[39m, in \u001b[36mVecISA.__bool__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__bool__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__bool__impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvec_isa_ok\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabdpt_env\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:152\u001b[39m, in \u001b[36mVecISA.__bool__impl\u001b[39m\u001b[34m(self, vec_isa_ok)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.is_fbcode():\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheck_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVecISA\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_avx_code\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabdpt_env\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:102\u001b[39m, in \u001b[36mVecISA.check_build\u001b[39m\u001b[34m(self, code)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcodecache\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_lock_dir, LOCK_TIMEOUT, write\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcpp_builder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     94\u001b[39m     CppBuilder,\n\u001b[32m     95\u001b[39m     CppTorchOptions,\n\u001b[32m     96\u001b[39m     normalize_path_separator,\n\u001b[32m     97\u001b[39m )\n\u001b[32m     99\u001b[39m key, input_path = write(\n\u001b[32m    100\u001b[39m     code,\n\u001b[32m    101\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcpp\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     extra=\u001b[43m_get_isa_dry_compile_fingerprint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_arch_flags\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    103\u001b[39m )\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_filelock\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FileLock\n\u001b[32m    106\u001b[39m lock_dir = get_lock_dir()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabdpt_env\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:28\u001b[39m, in \u001b[36m_get_isa_dry_compile_fingerprint\u001b[39m\u001b[34m(isa_flags)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_isa_dry_compile_fingerprint\u001b[39m(isa_flags: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# ISA dry compile will cost about 1 sec time each startup time.\u001b[39;00m\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# Please check the issue: https://github.com/pytorch/pytorch/issues/100378\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m     \u001b[38;5;66;03m# and generated them to output binary hash path.\u001b[39;00m\n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# It would optimize and skip compile existing binary.\u001b[39;00m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcpp_builder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_compiler_version_info, get_cpp_compiler\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     compiler_info = get_compiler_version_info(\u001b[43mget_cpp_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     29\u001b[39m     torch_version = torch.__version__\n\u001b[32m     30\u001b[39m     fingerprint = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompiler_info\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00misa_flags\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabdpt_env\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py:148\u001b[39m, in \u001b[36mget_cpp_compiler\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _IS_WINDOWS:\n\u001b[32m    147\u001b[39m     compiler = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mCXX\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     \u001b[43mcheck_compiler_exist_windows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.is_fbcode():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabdpt_env\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py:139\u001b[39m, in \u001b[36mcheck_compiler_exist_windows\u001b[39m\u001b[34m(compiler)\u001b[39m\n\u001b[32m    137\u001b[39m     subprocess.check_output([compiler, \u001b[33m\"\u001b[39m\u001b[33m/help\u001b[39m\u001b[33m\"\u001b[39m], stderr=subprocess.STDOUT)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCompiler: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompiler\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not found.\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess.SubprocessError:\n\u001b[32m    141\u001b[39m     \u001b[38;5;66;03m# Expected that some compiler(clang, clang++) is exist, but they not support `/help` args.\u001b[39;00m\n\u001b[32m    142\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[31mInductorError\u001b[39m: RuntimeError: Compiler: cl is not found.\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tabdpt import TabDPTRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error   # <-- use this call\n",
    "\n",
    "# 1 . sample 10 k rows for quick training\n",
    "train_sub = train_agg.sample(n=10_000, random_state=42)\n",
    "X_sub = train_sub.drop('Yield', axis=1).to_numpy()\n",
    "y_sub = train_sub['Yield'].to_numpy()\n",
    "\n",
    "# 2 . build the model\n",
    "model = TabDPTRegressor(\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "# 3 . fit\n",
    "model.fit(X_sub, y_sub)\n",
    "\n",
    "# 4 . evaluate (keep context_size ≤ 8192 so SDPA never overflows)\n",
    "for name, X_np, y_np in [\n",
    "    (\"Val\",  X_val.to_numpy(),  y_val.to_numpy()),\n",
    "    (\"Test\", X_test.to_numpy(), y_test.to_numpy())\n",
    "]:\n",
    "    preds = model.predict(X_np, context_size=4096, n_ensembles=1)\n",
    "    rmse  = mean_squared_error(y_np, preds, squared=False)  # root-MSE\n",
    "    print(f\"{name} R²   : {r2_score(y_np, preds):.4f}\")\n",
    "    print(f\"{name} RMSE : {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f89b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25755130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16941369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# %%\n",
    "# 1. Load\n",
    "train = pd.read_csv('train.csv')\n",
    "val   = pd.read_csv('val.csv')\n",
    "test  = pd.read_csv('test.csv')\n",
    "\n",
    "# %%\n",
    "# 1. Compute fill‐values on TRAIN only\n",
    "mg_mode      = train['MG'].mode()[0]           # mode for categorical MG\n",
    "lon_median   = train['Longitude'].median()     # median for longitude\n",
    "mean_cols    = ['Lodging','PlantHeight','SeedSize','Protein','Oil']\n",
    "mean_values  = train[mean_cols].mean()         # means for plant characteristics\n",
    "\n",
    "# 2. Fill missing in train/val/test\n",
    "for df in (train, val, test):\n",
    "    df['MG']        = df['MG'].fillna(mg_mode)\n",
    "    df['Longitude'] = df['Longitude'].fillna(lon_median)\n",
    "    for col in mean_cols:\n",
    "        df[col]     = df[col].fillna(mean_values[col])\n",
    "\n",
    "# 3. Define your features\n",
    "# %%\n",
    "# make sure these are defined at top‐level:\n",
    "temporal_feats = ['MaxTemp','MinTemp','AvgTemp','AvgHumidity','Precipitation','Radiation']\n",
    "static_feats   = ['Latitude','Longitude','Row.Spacing']\n",
    "plant_feats    = ['Lodging','PlantHeight','SeedSize','Protein','Oil']\n",
    "cluster_feats  = [f'Cluster_{i}' for i in range(40)]\n",
    "\n",
    "def aggregate_sequences(df, target='Yield', agg_target='mean'):\n",
    "    agg_dict = {}\n",
    "    # 1. temporal: mean & std\n",
    "    for feat in temporal_feats:\n",
    "        agg_dict[f'{feat}_mean'] = (feat, 'mean')\n",
    "        agg_dict[f'{feat}_std']  = (feat, 'std')\n",
    "    # 2. static geography: take first (constant per sequence)\n",
    "    for feat in static_feats:\n",
    "        agg_dict[feat] = (feat, 'first')\n",
    "    # 3. plant features:\n",
    "    #    - MG (categorical) → mode  \n",
    "    agg_dict['MG'] = ('MG', lambda x: x.mode().iloc[0])\n",
    "    #    - Lodging, PlantHeight, SeedSize, Protein, Oil → first\n",
    "    for feat in plant_feats:\n",
    "        agg_dict[feat] = (feat, 'first')\n",
    "    # 4. cluster indicators: proportion of time in each cluster + variability\n",
    "    for feat in cluster_feats:\n",
    "        agg_dict[f'{feat}_mean'] = (feat, 'mean')\n",
    "        agg_dict[f'{feat}_std']  = (feat, 'std')\n",
    "    # 5. target: mean or final\n",
    "    if agg_target == 'mean':\n",
    "        agg_dict[target] = (target, 'mean')\n",
    "    elif agg_target == 'final':\n",
    "        agg_dict[target] = (target, lambda x: x.iloc[-1])\n",
    "    else:\n",
    "        raise ValueError(\"agg_target must be 'mean' or 'final'\")\n",
    "    # apply the aggregation\n",
    "    grouped = df.groupby('TimeSeriesLabel').agg(**agg_dict)\n",
    "    return grouped.reset_index(drop=True)\n",
    "\n",
    "# %%\n",
    "train_agg = aggregate_sequences(train, agg_target='mean')\n",
    "val_agg   = aggregate_sequences(val,   agg_target='mean')\n",
    "test_agg  = aggregate_sequences(test,  agg_target='mean')\n",
    "\n",
    "# %%\n",
    "# 5. Split features / target\n",
    "X_train = train_agg.drop('Yield', axis=1)\n",
    "y_train = train_agg['Yield']\n",
    "X_val   = val_agg.drop('Yield',   axis=1)\n",
    "y_val   = val_agg['Yield']\n",
    "X_test  = test_agg.drop('Yield',  axis=1)\n",
    "y_test  = test_agg['Yield']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c2156f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data and preparing for CPU-only execution...\n",
      "Using 2000 samples for training\n",
      "Initializing TabDPT model on CPU...\n",
      "Training model...\n",
      "Model training completed successfully!\n",
      "\n",
      "Evaluating model...\n",
      "\n",
      "Validation Set Evaluation:\n",
      "Predicting on 10763 samples...\n",
      "Batch 0 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 1 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 2 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 3 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 4 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 5 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 6 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 7 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 8 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 9 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 10 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 11 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 12 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 13 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 14 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 15 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 16 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 17 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 18 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 19 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 20 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 21 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 22 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 23 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 24 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 25 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 26 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 27 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 28 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 29 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 30 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 31 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 32 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 33 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 34 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 35 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 36 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 37 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 38 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 39 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 40 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 41 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 42 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 43 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 44 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 45 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 46 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 47 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 48 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 49 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 50 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 51 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 52 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 53 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 54 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 55 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 56 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 57 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 58 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 59 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 60 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 61 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 62 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 63 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 64 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 65 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 66 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 67 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 68 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 69 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 70 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 71 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 72 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 73 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 74 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 75 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 76 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 77 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 78 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 79 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 80 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 81 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 82 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 83 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 84 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 85 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 86 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 87 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 88 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 89 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 90 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 91 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 92 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 93 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 94 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 95 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 96 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 97 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 98 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 99 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 100 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 101 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 102 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 103 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 104 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 105 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 106 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 107 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 108 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 109 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 110 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 111 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 112 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 113 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 114 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 115 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 116 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 117 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 118 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 119 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 120 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 121 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 122 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 123 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 124 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 125 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 126 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 127 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 128 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 129 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 130 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 131 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 132 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 133 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 134 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 135 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 136 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 137 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 138 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 139 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 140 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 141 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 142 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 143 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 144 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 145 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 146 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 147 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 148 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 149 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 150 failed even with context_size=1: FlashAttention requires CUDA support\n",
      "Batch 151 failed even with context_size=1: FlashAttention requires CUDA support\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 135\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;66;03m# Get predictions\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPredicting on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_np)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m samples...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m predictions = \u001b[43msafe_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m# Calculate metrics only for successful predictions\u001b[39;00m\n\u001b[32m    138\u001b[39m valid_mask = predictions != \u001b[32m0\u001b[39m  \u001b[38;5;66;03m# Remove placeholder predictions\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 104\u001b[39m, in \u001b[36msafe_predict\u001b[39m\u001b[34m(model, X_data, batch_size)\u001b[39m\n\u001b[32m    101\u001b[39m     batch = batch.astype(np.float32)\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# Predict with minimal settings\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m preds = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_ensembles\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m    108\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m predictions.extend(preds)\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Success, move to next batch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\TabDPT\\src\\tabdpt\\regressor.py:94\u001b[39m, in \u001b[36mTabDPTRegressor.predict\u001b[39m\u001b[34m(self, X, n_ensembles, context_size, seed)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\n\u001b[32m     87\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     88\u001b[39m     X: np.ndarray,\n\u001b[32m   (...)\u001b[39m\u001b[32m     91\u001b[39m     seed: \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     92\u001b[39m ):\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_ensembles == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     96\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._ensemble_predict(X, n_ensembles=n_ensembles, context_size=context_size)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabdpt_env\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\TabDPT\\src\\tabdpt\\regressor.py:35\u001b[39m, in \u001b[36mTabDPTRegressor._predict\u001b[39m\u001b[34m(self, X, context_size, seed)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;129m@torch\u001b[39m.no_grad()\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_predict\u001b[39m(\n\u001b[32m     30\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m     seed: \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     34\u001b[39m ):\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     train_x, train_y, test_x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     38\u001b[39m         feat_perm = generate_random_permutation(train_x.shape[\u001b[32m1\u001b[39m], seed)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\TabDPT\\src\\tabdpt\\estimator.py:89\u001b[39m, in \u001b[36mTabDPTEstimator._prepare_prediction\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# Apply PCA optionally to reduce the number of features\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_features > \u001b[38;5;28mself\u001b[39m.max_features:\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     _, _, \u001b[38;5;28mself\u001b[39m.V = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpca_lowrank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m     train_x = train_x @ \u001b[38;5;28mself\u001b[39m.V\n\u001b[32m     91\u001b[39m     test_x = test_x @ \u001b[38;5;28mself\u001b[39m.V\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabdpt_env\\Lib\\site-packages\\torch\\_lowrank.py:294\u001b[39m, in \u001b[36mpca_lowrank\u001b[39m\u001b[34m(A, q, center, niter)\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    293\u001b[39m     C = A.mean(dim=(-\u001b[32m2\u001b[39m,), keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_svd_lowrank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mniter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mniter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabdpt_env\\Lib\\site-packages\\torch\\_lowrank.py:170\u001b[39m, in \u001b[36m_svd_lowrank\u001b[39m\u001b[34m(A, q, niter, M)\u001b[39m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m M \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    168\u001b[39m         M = M.mH\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m Q = \u001b[43mget_approximate_basis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mniter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mniter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m=\u001b[49m\u001b[43mM\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m B = matmul(Q.mH, A)\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m M \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabdpt_env\\Lib\\site-packages\\torch\\_lowrank.py:73\u001b[39m, in \u001b[36mget_approximate_basis\u001b[39m\u001b[34m(A, q, niter, M)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m M \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     72\u001b[39m     X = X - matmul(M, R)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m Q = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mqr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m.Q\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(niter):\n\u001b[32m     75\u001b[39m     X = matmul(A.mH, Q)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# SOLUTION: Completely disable PyTorch compilation and use CPU\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Disable ALL compilation attempts\n",
    "os.environ['TORCH_COMPILE_DISABLE'] = '1'\n",
    "os.environ['TORCHINDUCTOR_DISABLE'] = '1'\n",
    "os.environ['TORCH_CUDNN_V8_API_DISABLED'] = '1'\n",
    "os.environ['PYTORCH_DISABLE_LIBRARY'] = '1'\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "import torch._dynamo\n",
    "import torch._inductor\n",
    "\n",
    "# Disable dynamo and inductor completely\n",
    "torch._dynamo.config.disable = True\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "torch._inductor.config.disable_progress = True\n",
    "torch._inductor.config.triton.cudagraphs = False\n",
    "\n",
    "# Force CPU backend\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import gc\n",
    "\n",
    "# Import TabDPT AFTER disabling compilation\n",
    "from tabdpt import TabDPTRegressor\n",
    "\n",
    "# %%\n",
    "# Clear any GPU memory and force CPU\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    # Explicitly disable CUDA\n",
    "    torch.cuda.is_available = lambda: False\n",
    "\n",
    "# %%\n",
    "print(\"Loading data and preparing for CPU-only execution...\")\n",
    "\n",
    "# Your existing data should be loaded here\n",
    "# train_agg, val_agg, test_agg, X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# %%\n",
    "# Create a small training subset\n",
    "n_samples = min(2000, len(train_agg))\n",
    "print(f\"Using {n_samples} samples for training\")\n",
    "\n",
    "train_sub = train_agg.sample(n=n_samples, random_state=42)\n",
    "X_sub = train_sub.drop('Yield', axis=1).to_numpy().astype(np.float32)\n",
    "y_sub = train_sub['Yield'].to_numpy().astype(np.float32)\n",
    "\n",
    "# %%\n",
    "# Initialize model - FORCE CPU\n",
    "print(\"Initializing TabDPT model on CPU...\")\n",
    "model = TabDPTRegressor(device=\"cpu\")\n",
    "\n",
    "# Double-check it's on CPU\n",
    "if hasattr(model, 'model'):\n",
    "    for param in model.model.parameters():\n",
    "        if param.is_cuda:\n",
    "            param.data = param.data.cpu()\n",
    "\n",
    "# %%\n",
    "# Fit the model\n",
    "print(\"Training model...\")\n",
    "try:\n",
    "    model.fit(X_sub, y_sub)\n",
    "    print(\"Model training completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Training error: {e}\")\n",
    "    print(\"\\nTrying with even smaller dataset...\")\n",
    "    # Try with just 500 samples\n",
    "    X_mini = X_sub[:500]\n",
    "    y_mini = y_sub[:500]\n",
    "    model.fit(X_mini, y_mini)\n",
    "    print(\"Model trained on reduced dataset\")\n",
    "\n",
    "# %%\n",
    "# Define safe prediction function\n",
    "def safe_predict(model, X_data, batch_size=50):\n",
    "    \"\"\"Predict in small batches with error handling\"\"\"\n",
    "    n_samples = len(X_data)\n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        end_idx = min(i + batch_size, n_samples)\n",
    "        batch = X_data[i:end_idx]\n",
    "        \n",
    "        # Multiple attempts with decreasing context size\n",
    "        for context_size in [64, 32, 16, 8, 4, 1]:\n",
    "            try:\n",
    "                # Ensure batch is on CPU and correct dtype\n",
    "                if isinstance(batch, np.ndarray):\n",
    "                    batch = batch.astype(np.float32)\n",
    "                \n",
    "                # Predict with minimal settings\n",
    "                preds = model.predict(\n",
    "                    batch,\n",
    "                    context_size=min(context_size, len(batch)),\n",
    "                    n_ensembles=1\n",
    "                )\n",
    "                \n",
    "                predictions.extend(preds)\n",
    "                break  # Success, move to next batch\n",
    "                \n",
    "            except Exception as e:\n",
    "                if context_size == 1:\n",
    "                    print(f\"Batch {i//batch_size} failed even with context_size=1: {str(e)[:100]}\")\n",
    "                    # Skip this batch\n",
    "                    predictions.extend([0] * len(batch))  # Placeholder predictions\n",
    "                continue\n",
    "    \n",
    "    return np.array(predictions, dtype=np.float32)\n",
    "\n",
    "# %%\n",
    "# Evaluate on validation and test sets\n",
    "print(\"\\nEvaluating model...\")\n",
    "\n",
    "for name, X_eval, y_eval in [(\"Validation\", X_val, y_val), (\"Test\", X_test, y_test)]:\n",
    "    print(f\"\\n{name} Set Evaluation:\")\n",
    "    \n",
    "    # Convert to numpy and ensure float32\n",
    "    X_np = X_eval.to_numpy().astype(np.float32)\n",
    "    y_np = y_eval.to_numpy().astype(np.float32)\n",
    "    \n",
    "    # Get predictions\n",
    "    print(f\"Predicting on {len(X_np)} samples...\")\n",
    "    predictions = safe_predict(model, X_np, batch_size=25)\n",
    "    \n",
    "    # Calculate metrics only for successful predictions\n",
    "    valid_mask = predictions != 0  # Remove placeholder predictions\n",
    "    if valid_mask.sum() > 0:\n",
    "        valid_preds = predictions[valid_mask]\n",
    "        valid_y = y_np[valid_mask]\n",
    "        \n",
    "        rmse = mean_squared_error(valid_y, valid_preds, squared=False)\n",
    "        r2 = r2_score(valid_y, valid_preds)\n",
    "        \n",
    "        print(f\"Successfully predicted {valid_mask.sum()}/{len(y_np)} samples\")\n",
    "        print(f\"{name} R²   : {r2:.4f}\")\n",
    "        print(f\"{name} RMSE : {rmse:.4f}\")\n",
    "    else:\n",
    "        print(f\"Failed to get valid predictions for {name} set\")\n",
    "\n",
    "# %%\n",
    "# Alternative: If TabDPT still fails, here's a minimal working predictor\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"If TabDPT continues to fail, here's a simple baseline:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Train a simple Random Forest as baseline\n",
    "rf_model = RandomForestRegressor(n_estimators=50, max_depth=10, random_state=42, n_jobs=1)\n",
    "rf_model.fit(X_sub, y_sub)\n",
    "\n",
    "# Evaluate RF baseline\n",
    "for name, X_eval, y_eval in [(\"Validation\", X_val, y_val), (\"Test\", X_test, y_test)]:\n",
    "    preds = rf_model.predict(X_eval)\n",
    "    rmse = mean_squared_error(y_eval, preds, squared=False)\n",
    "    r2 = r2_score(y_eval, preds)\n",
    "    print(f\"\\nRandom Forest {name} - R²: {r2:.4f}, RMSE: {rmse:.4f}\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TROUBLESHOOTING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(\"The 'cl is not found' error occurs because:\")\n",
    "print(\"1. PyTorch is trying to compile custom CUDA kernels\")\n",
    "print(\"2. Microsoft Visual C++ compiler (cl.exe) is not installed\")\n",
    "print(\"3. This is a Windows-specific issue\")\n",
    "print(\"\\nThis script forces CPU-only execution and disables compilation.\")\n",
    "print(\"\\nIf you still get errors:\")\n",
    "print(\"1. Restart your Python kernel/interpreter\")\n",
    "print(\"2. Make sure you run the environment setup BEFORE importing torch\")\n",
    "print(\"3. Consider using WSL2 or Linux where these issues don't occur\")\n",
    "print(\"4. Or use the Random Forest baseline which always works\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dc4c49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabdpt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
