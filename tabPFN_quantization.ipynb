{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34e73d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabpfn import TabPFNRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# 1. Load\n",
    "train = pd.read_csv('train.csv')\n",
    "val   = pd.read_csv('val.csv')\n",
    "test  = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d06e53a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0034e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Load raw CSVs\n",
    "train = pd.read_csv('train.csv')\n",
    "val   = pd.read_csv('val.csv')\n",
    "test  = pd.read_csv('test.csv')\n",
    "\n",
    "# 2. Impute missing values (TRAIN-only statistics)\n",
    "mg_mode    = train['MG'].mode()[0]\n",
    "lon_med    = train['Longitude'].median()\n",
    "mean_cols  = ['Lodging','PlantHeight','SeedSize','Protein','Oil']\n",
    "mean_vals  = train[mean_cols].mean()\n",
    "\n",
    "for df in (train, val, test):\n",
    "    df['MG']        = df['MG'].fillna(mg_mode)\n",
    "    df['Longitude'] = df['Longitude'].fillna(lon_med)\n",
    "    for c in mean_cols:\n",
    "        df[c]      = df[c].fillna(mean_vals[c])\n",
    "\n",
    "# 3. Feature definitions\n",
    "temporal_feats = ['MaxTemp','MinTemp','AvgTemp','AvgHumidity','Precipitation','Radiation']\n",
    "static_feats   = ['Latitude','Longitude','Row.Spacing']\n",
    "plant_feats    = ['Lodging','PlantHeight','SeedSize','Protein','Oil']\n",
    "cluster_feats  = [f'Cluster_{i}' for i in range(40)]\n",
    "\n",
    "# 4. Aggregation function\n",
    "def aggregate_sequences(df, target='Yield', agg_target='mean'):\n",
    "    agg = {}\n",
    "    # temporal: mean & std\n",
    "    for f in temporal_feats:\n",
    "        agg[f'{f}_mean'] = (f, 'mean')\n",
    "        agg[f'{f}_std']  = (f, 'std')\n",
    "    # static geography: first\n",
    "    for f in static_feats:\n",
    "        agg[f] = (f, 'first')\n",
    "    # plant: MG by mode, others by first\n",
    "    agg['MG'] = ('MG', lambda x: x.mode().iloc[0])\n",
    "    for f in plant_feats:\n",
    "        agg[f] = (f, 'first')\n",
    "    # clusters: mean & std\n",
    "    for f in cluster_feats:\n",
    "        agg[f'{f}_mean'] = (f, 'mean')\n",
    "        agg[f'{f}_std']  = (f, 'std')\n",
    "    # target\n",
    "    if agg_target=='mean':\n",
    "        agg[target] = (target, 'mean')\n",
    "    else:\n",
    "        agg[target] = (target, lambda x: x.iloc[-1])\n",
    "    return df.groupby('TimeSeriesLabel').agg(**agg).reset_index(drop=True)\n",
    "\n",
    "train_agg = aggregate_sequences(train)\n",
    "val_agg   = aggregate_sequences(val)\n",
    "test_agg  = aggregate_sequences(test)\n",
    "\n",
    "# 5. Split into X/y\n",
    "X_train = train_agg.drop('Yield', axis=1)\n",
    "y_train = train_agg['Yield']\n",
    "\n",
    "X_val   = val_agg.drop('Yield',   axis=1)\n",
    "y_val   = val_agg['Yield']\n",
    "\n",
    "X_test  = test_agg.drop('Yield',  axis=1)\n",
    "y_test  = test_agg['Yield']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f406a73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Quantization helpers\n",
    "def fit_quantizer(df, b):\n",
    "    levels = 2**b\n",
    "    params = {}\n",
    "    for col in df.columns:\n",
    "        xmin, xmax = df[col].min(), df[col].max()\n",
    "        if xmax==xmin: continue\n",
    "        delta = (xmax-xmin)/(levels-1)\n",
    "        params[col] = (xmin, delta)\n",
    "    return params\n",
    "\n",
    "def apply_quantizer(df, params):\n",
    "    df_q = df.copy()\n",
    "    for col, (xmin, delta) in params.items():\n",
    "        df_q[col] = xmin + delta * np.round((df_q[col]-xmin)/delta)\n",
    "    return df_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c9d253a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b=2 → Val R²: 0.5212, RMSE: 10.3723\n",
      "b=2 → Test R²: 0.5267, RMSE: 10.2425\n",
      "----------------------------------------\n",
      "b=4 → Val R²: 0.7651, RMSE: 7.2655\n",
      "b=4 → Test R²: 0.7680, RMSE: 7.1704\n",
      "----------------------------------------\n",
      "b=6 → Val R²: 0.7741, RMSE: 7.1243\n",
      "b=6 → Test R²: 0.7779, RMSE: 7.0164\n",
      "----------------------------------------\n",
      "b=8 → Val R²: 0.7772, RMSE: 7.0752\n",
      "b=8 → Test R²: 0.7790, RMSE: 6.9979\n",
      "----------------------------------------\n",
      "b=16 → Val R²: 0.7775, RMSE: 7.0705\n",
      "b=16 → Test R²: 0.7797, RMSE: 6.9869\n",
      "----------------------------------------\n",
      "b=32 → Val R²: 0.7775, RMSE: 7.0698\n",
      "b=32 → Test R²: 0.7798, RMSE: 6.9860\n",
      "----------------------------------------\n",
      "b=64 → Val R²: 0.7775, RMSE: 7.0698\n",
      "b=64 → Test R²: 0.7798, RMSE: 6.9862\n",
      "----------------------------------------\n",
      "b=128 → Val R²: 0.7775, RMSE: 7.0698\n",
      "b=128 → Test R²: 0.7798, RMSE: 6.9862\n",
      "----------------------------------------\n",
      "b=256 → Val R²: 0.7775, RMSE: 7.0698\n",
      "b=256 → Test R²: 0.7798, RMSE: 6.9862\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 7. Loop over bit-depths and evaluate\n",
    "for b in [2,4,6,8,16,32,64,128,256]:\n",
    "    # learn grid on X_train\n",
    "    q_params = fit_quantizer(X_train, b)\n",
    "    Xtr_q = apply_quantizer(X_train, q_params).to_numpy()\n",
    "    Xvl_q = apply_quantizer(X_val,   q_params).to_numpy()\n",
    "    Xte_q = apply_quantizer(X_test,  q_params).to_numpy()\n",
    "\n",
    "    # subsample 10k for fitting\n",
    "    idx    = np.random.RandomState(42).choice(len(Xtr_q),  size=10_000, replace=False)\n",
    "    Xsub, ysub = Xtr_q[idx], y_train.to_numpy()[idx]\n",
    "\n",
    "    model = TabPFNRegressor()\n",
    "    model.fit(Xsub, ysub)\n",
    "\n",
    "    # evaluate\n",
    "    for name, Xq, y in [('Val', Xvl_q, y_val.to_numpy()),\n",
    "                       ('Test', Xte_q, y_test.to_numpy())]:\n",
    "        preds = model.predict(Xq)\n",
    "        print(f\"b={b} → {name} R²: {r2_score(y,preds):.4f}, RMSE: {root_mean_squared_error(y,preds):.4f}\")\n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec93f657",
   "metadata": {},
   "source": [
    "#Applying Autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b94dbbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250726_065718\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.23\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22631\n",
      "CPU Count:          32\n",
      "Memory Avail:       23.10 GB / 63.70 GB (36.3%)\n",
      "Disk Space Avail:   651.85 GB / 1883.61 GB (34.6%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='experimental' : New in v1.2: Pre-trained foundation model + parallel fits. The absolute best accuracy without consideration for inference speed. Does not support GPU.\n",
      "\tpresets='best'         : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'         : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'         : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'       : Fast training time, ideal for initial prototyping.\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (96864 samples, 79.82 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"c:\\Users\\mohd7\\TabPFN-data\\Weather\\AutogluonModels\\ag-20250726_065718\"\n",
      "Train Data Rows:    96864\n",
      "Train Data Columns: 101\n",
      "Label Column:       Yield\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    23655.09 MB\n",
      "\tTrain Data (Original)  Memory Usage: 74.64 MB (0.3% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 40 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 40): ['Cluster_0_std', 'Cluster_1_std', 'Cluster_2_std', 'Cluster_3_std', 'Cluster_4_std', 'Cluster_5_std', 'Cluster_6_std', 'Cluster_7_std', 'Cluster_8_std', 'Cluster_9_std', 'Cluster_10_std', 'Cluster_11_std', 'Cluster_12_std', 'Cluster_13_std', 'Cluster_14_std', 'Cluster_15_std', 'Cluster_16_std', 'Cluster_17_std', 'Cluster_18_std', 'Cluster_19_std', 'Cluster_20_std', 'Cluster_21_std', 'Cluster_22_std', 'Cluster_23_std', 'Cluster_24_std', 'Cluster_25_std', 'Cluster_26_std', 'Cluster_27_std', 'Cluster_28_std', 'Cluster_29_std', 'Cluster_30_std', 'Cluster_31_std', 'Cluster_32_std', 'Cluster_33_std', 'Cluster_34_std', 'Cluster_35_std', 'Cluster_36_std', 'Cluster_37_std', 'Cluster_38_std', 'Cluster_39_std']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 61 | ['MaxTemp_mean', 'MaxTemp_std', 'MinTemp_mean', 'MinTemp_std', 'AvgTemp_mean', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 21 | ['MaxTemp_mean', 'MaxTemp_std', 'MinTemp_mean', 'MinTemp_std', 'AvgTemp_mean', ...]\n",
      "\t\t('int', ['bool']) : 40 | ['Cluster_0_mean', 'Cluster_1_mean', 'Cluster_2_mean', 'Cluster_3_mean', 'Cluster_4_mean', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t61 features in original data used to generate 61 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 19.21 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.5s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.025809382226627025, Train Rows: 94364, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-6.7621\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-6.7501\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 6.88191\n",
      "[2000]\tvalid_set's rmse: 6.49003\n",
      "[3000]\tvalid_set's rmse: 6.32587\n",
      "[4000]\tvalid_set's rmse: 6.22362\n",
      "[5000]\tvalid_set's rmse: 6.16246\n",
      "[6000]\tvalid_set's rmse: 6.11801\n",
      "[7000]\tvalid_set's rmse: 6.08595\n",
      "[8000]\tvalid_set's rmse: 6.06226\n",
      "[9000]\tvalid_set's rmse: 6.04346\n",
      "[10000]\tvalid_set's rmse: 6.02306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-6.0227\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.52s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 6.51626\n",
      "[2000]\tvalid_set's rmse: 6.20191\n",
      "[3000]\tvalid_set's rmse: 6.07512\n",
      "[4000]\tvalid_set's rmse: 6.03353\n",
      "[5000]\tvalid_set's rmse: 6.00412\n",
      "[6000]\tvalid_set's rmse: 6.00156\n",
      "[7000]\tvalid_set's rmse: 5.98794\n",
      "[8000]\tvalid_set's rmse: 5.98226\n",
      "[9000]\tvalid_set's rmse: 5.98325\n",
      "[10000]\tvalid_set's rmse: 5.98357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-5.9796\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.38s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-6.2361\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.41s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-6.0113\t = Validation score   (-root_mean_squared_error)\n",
      "\t70.07s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-6.0757\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.74s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.3.1`. \n",
      "Fitting model: XGBoost ...\n",
      "\t-6.0171\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.3s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\t__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabpfn_env\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"c:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabpfn_env\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"c:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabpfn_env\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabpfn_env\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 209, in _fit\n",
      "    train_dataset = self._generate_dataset(X=X, y=y, train_params=processor_kwargs, is_train=True)\n",
      "  File \"c:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabpfn_env\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 687, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "  File \"c:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabpfn_env\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 749, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "  File \"c:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\tabpfn_env\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "TypeError: __init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 6.21494\n",
      "[2000]\tvalid_set's rmse: 6.09353\n",
      "[3000]\tvalid_set's rmse: 6.05084\n",
      "[4000]\tvalid_set's rmse: 6.04852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-6.0423\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.85s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'LightGBM': 0.238, 'XGBoost': 0.238, 'LightGBMXT': 0.19, 'ExtraTreesMSE': 0.143, 'KNeighborsDist': 0.095, 'RandomForestMSE': 0.048, 'CatBoost': 0.048}\n",
      "\t-5.8738\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 151.47s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 7562.6 rows/s (2500 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\mohd7\\TabPFN-data\\Weather\\AutogluonModels\\ag-20250726_065718\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AutoGluon] R²: 0.8462, RMSE: 5.8391\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "# Prepare train/val/test dataframes by joining X and y\n",
    "train_autogluon = pd.concat([X_train, y_train.rename(\"Yield\")], axis=1)\n",
    "val_autogluon   = pd.concat([X_val,   y_val.rename(\"Yield\")], axis=1)\n",
    "test_autogluon  = pd.concat([X_test,  y_test.rename(\"Yield\")], axis=1)\n",
    "\n",
    "# Combine train + val for stronger training\n",
    "full_train = pd.concat([train_autogluon, val_autogluon], axis=0)\n",
    "\n",
    "# Train AutoGluon Predictor\n",
    "predictor = TabularPredictor(label='Yield', problem_type='regression').fit(train_data=full_train)\n",
    "\n",
    "# Evaluate\n",
    "preds = predictor.predict(test_autogluon.drop(columns=['Yield']))\n",
    "r2 = r2_score(test_autogluon['Yield'], preds)\n",
    "rmse = np.sqrt(mean_squared_error(test_autogluon['Yield'], preds))\n",
    "\n",
    "print(f\"[AutoGluon] R²: {r2:.4f}, RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9117747",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabpfn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
